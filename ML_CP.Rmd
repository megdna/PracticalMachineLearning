---
title: "Practical Machine Learning - Course Project Writeup"
author: "Megan"
date: "Sunday, July 26, 2015"
output: 
  html_document:
    keep_md: true
---

#### **Components of a predictor:** question -> data -> features -> algorithm -> parameters -> evaluation

## Question

The goal of this project is to predict the manner in which 6 participants performed barbell lifts (correctly and incorrectly in 5 different ways) using data collected from accelerometers on the belt, forearm, arm and dumbbell.  In other words, this prediction model will attempt to determine how well an activity was performed by the wearer.

## Input Data

Download the [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) datasets obtained from the [Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) into the working directory. Then, read the local files into R:

```{r}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

## Features

Explore the data to determine which features are relevant in predicting activity quality (classe) from activity monitors:

```{r}
str(training[,1:45])
```

As observed, the initial descriptive variables (columns 1-7) and those containing missing values (including factor variables) are not useful for building this model. Therefore, select only the appropriate columns from the datasets:

```{r}
train <- training[,grep("^roll|^pitch|^yaw|^total|^gyros|^accel|^magnet|classe", names(training))]
test <- testing[,grep("^roll|^pitch|^yaw|^total|^gyros|^accel|^magnet|problem", names(testing))]
```

The remaining 52 predictors (down from 159) represent different angles of movement (roll, pitch and yaw) and 3-axis inertial measurements (acceleration, gyroscope and magnetometer) for 4 sensors' orientation (belt, arm, dumbbell and forearm). None of these covariates have (near) zero variance:

```{r message=FALSE, warning=FALSE}
library(caret)
library(knitr)
kable(nearZeroVar(train, saveMetrics=TRUE), align="l")
```

For exploratory purposes, plot the total acceleration for all sensors:

```{r PairsPlot, fig.height=10, fig.width=10}
featurePlot(x=train[,c(4,17,30,43)], y=train$classe, plot="pairs")
```

Not surprisingly, the belt distribution appears to be different from the other sensors measuring bicep curls. Moreover, there does not appear to be a clear linear relationship.

Evaluate just the dumbbell for illustration:

```{r DensityPlot, fig.height=5, fig.width=10}
qplot(total_accel_dumbbell, color=classe, data=train, geom="density")
```

Interestingly, there is a different pattern of acceleration for each class:

- A - exercise performed exactly according to the specification
- B - throwing the elbows to the front
- C - lifting the dumbbell only halfway
- D - lowering the dumbbell only halfway
- E - throwing the hips to the front

## Algorithm

Based on these insights, a non-linear machine learning algorithm will be developed without data transformations or principal component analysis (PCA) pre-processing. Since accuracy and performance are important for this assignment, random forests and boosting approaches will be explored.

Use the train set to split data further into 75% trainCV and 25% testCV for cross-validation:

```{r}
set.seed(2873)
inTrain <- createDataPartition(y=train$classe, p=0.75, list=FALSE)
trainCV <- train[inTrain,]
testCV <- train[-inTrain,]
```

Build model on the trainCV data:

```{r message=FALSE, warning=FALSE}
set.seed(2378)
library(randomForest)
RFM <-randomForest(classe ~ ., data=trainCV)
RFM
```

## Evaluation

Perform validation on the testCV set:

```{r}
CM <- confusionMatrix(testCV$classe, predict(RFM, testCV))
CM
```

```{r ConfusionMatrix, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10}
library(som)
library(reshape2)

CMN <- normalize(CM$table)
colnames(CMN) <- rownames(CMN)
CMN <- melt(CMN)

g <- ggplot(CMN)
g + geom_tile(aes(x=Var2, y=Prediction, fill=value)) + scale_x_discrete(name="Actual Class") +
        scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-0.5, to=1.8, by=.05)) +
        guides(fill=guide_legend(ncol=4)) + labs(title="Normalized Confusion Matrix for Random Forest Model") +
        labs(fill="Normalized\nFrequency")
```

The prediction accuracy of this model is estimated at 0.9951 or 99.51%, so the expected out of sample error is 1 - 0.9951 or 0.49%.  

*Note: with random forests there is no need for cross-validation or a separate test set because the out-of-bag (oob) estimate is an unbiased estimate of the test set error.*


```{r}
answers <- as.character(predict(RFM, test))
answer <- t(data.frame(answers))
colnames(answer) <- 1:20
kable(answer)
```

The actual out of sample error (fraction incorrect in categorical outcomes for the original test set) was 0/20 or 0% with 100% accuracy. This is likely due to the small size of the data partition.

*Note: other machine learning algorithms were not included in this report because they did not perform as well or better.*